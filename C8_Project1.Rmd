---
title: "Predicting Exercise Quality in the WLE Dataset"
author: "Justin Z"
date: "May 26, 2019"
output: html_document
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

ConvertWLE <- function(data) {
  # Converts data types of the WLE dataset variables, known to introduce NAs
  #
  # Args:
  #   data: a data object to be adjusted
  #
  # Returns:
  #   The data object where the data types of variables are converted
  suppressWarnings(mutate_at(data, c(8:159), as.double)) %>%  # NAs to 33 cols
    mutate_at(c(2, 5:6, 160), as.factor)
}

SubsetWLE <- function(data) {
  # Subsets data frames of the WLE dataset to prepare them for analysis
  #
  # Args:
  #   data: a data object to be subset
  #
  # Returns:
  #   The subsetted data object
  data %>%
  select(-na.columns) %>%
  select(c(-1, -(3:7)))
}

```


## Overview

This report is for the peer-graded project in the Practical Machine Learning
course from Johns Hopkins University within the Data Science Specialization on
Coursera. The instructions say to use the WLE dataset to build a model that
predicts the `classe` variable using any of the other variables in the dataset.
The model must then be used to predict the outcome of 20 different test cases.

To meet the objective, the data were loaded into R and split into training and
validation sets so that cross validation could be used to evaluate the model.
The data were then explored and processed, and then a model was fit to the
training subset using the random forest method. The model was evaluated on the
validation set, and the expected out of sample error rate is 0.41%. Lastly, the
model was used to predict the test cases.


## Acknowledgement

Acknowledgement for the WLE dataset goes to:

>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative
Activity Recognition of Weight Lifting Exercises. Proceedings of 4th
International Conference in Cooperation with SIGCHI (Augmented Human '13) .
Stuttgart, Germany: ACM SIGCHI, 2013.

Additional information on their project can be found [here][1].

[1]: http://groupware.les.inf.puc-rio.br/har "WLE Site"


## Part 1) Loading and Pre-processing the Data

In the code chunk below the required packages are loaded along with the training
data. Message output is suppressed to conserve space. Additionally, 2 functions
were written to facilitate data processing in this project, and the code for
these functions is not shown either. For those interested, unabriged analysis
can be viewed in the script in the [GitHub repo][2] for this project submission.

[2]: https://github.com/jtzingsheim1/WLE-Predicting-Classe "GitHub repo"

```{r loading, message = FALSE}

# Messages suppressed for this code chunk
library(tidyverse)
library(caret)

# Check if the file exists in the directory before downloading it again
training.file <- "pml-training.csv"
if (!file.exists(training.file)) {
  url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(url, training.file)
  rm(url)
}

# Load in the raw training data
raw.train.data <- read.csv(training.file, stringsAsFactors = F)  # 19622 of 160

# Next split off a validation data set from the training data
set.seed(190522)
in.train <- createDataPartition(y = raw.train.data$classe, p = 0.9, list = F)
train.subset <- raw.train.data[in.train, ]  # 17662 obs. of 160 variables
validation.data <- raw.train.data[-in.train, ]  # 1960 obs. of 160 variables
rm(training.file, in.train)

```


## Part 2) Explore and Process Data

In the next code chunk a portion of the data are previewed with `str()`, and it
can be seen that some factor variables like `user_name` and `classe` were loaded
as character type and need to be converted. Also, beginning at column 12 some
numeric variables were loaded as character type, and these also need conversion.
It can also be seen that many of these values are simply blank. A function was
created to do the type conversions and easily repeat them later on the
validation and test sets. When the blanks are converted to numeric it introduces
`NA`s by coercion and these warnings are suppressed in the function.

```{r exploring}

# Check out the data
str(train.subset[1:15])
# Convert some character columns to factor and some to numeric
train.subset <- ConvertWLE(train.subset)  # 17662 obs of 160 variables
# Check the data again
summary(train.subset[6:12])  # Some columns have NAs
# Check for NA values
na.fractions <- train.subset %>%
  map_dbl(function(x) {mean(is.na(x))}) %>%  # Calc. NA fraction of each column
  subset(. != 0)  # Named num [1:100], the 96 calculated features and 4 var_acc
summary(na.fractions)  # Min. = 0.9796, Max. = 1.0000

```

After checking the results of `summary()` it can be seen that some of the
variables contain large fractions of `NA` values. Of the 160 original variables
in the data, 100 of them contain `NA` values, and the fraction of `NA`s in each
of these columns is over 97%. By reading the research paper, one can find that
the authors defined blocks of sequential observations as "windows" of data and
calculated features for the window. These window level summaries represent a
second type of data but are stored alongside the measurement data.

Before proceeding it should be determined which type of data should be used as
the basis for prediction. By viewing the test data set it can be seen that
individual data points are provided (not window summaries). Since the prediction
model will not be based on window summary data, the variables that contain only
this data can be excluded.

```{r processing}

# Remove window summary columns from the training subset data
na.columns <- names(na.fractions)  # Get the names of the NA columns
train.subset <- select(train.subset, -na.columns)  # 17622 obs. of 60 variables
rm(na.fractions)
# Remove additional unneeded columns
train.subset <- select(train.subset, c(-1, -(3:7)))  # 17622 obs of 54 variables

```

After removing the 100 summary columns, the data were in tidy format, but still
contained variables that could cause problems or unnecessary complication in the
model fitting step. The remaining variables were assessed, and it was determined
that six columns could be removed from the training data. Additional detail on
the justification for these eliminations can be found in the script for this
project in the [GitHub repo][2]. Once the unneeded variables were removed in the
chunk above, the data were in tidy format and ready for model fitting.


## Part 3) Model Fitting

By reading the research paper, one can see that the authors used the random
forest method when building their own models, so that is clearly a good
starting point for this step. For this project, the random forest method was
used first, and the need for alternative methods was assessed based its
performance. Before proceeding the data were also checked for imbalance among
the factor variables which could cause an issue for the random forest technique.

```{r fitting, cache = TRUE}

par(mfrow = c(1, 2))  # Setup plot space
plot(train.subset$user_name)  # 6 levels, reasonably even distribution
plot(train.subset$classe)  # 5 levels, reasonably even distribution

# Extract predictors and responses to reduce calculation time
predictors <- select(train.subset, -classe)  # 17662 obs. of 53 variables
response <- train.subset$classe  # Factor with 5 levels, 17662 long
rf.model1 <- train(x = predictors, y = response, method = "rf")
print(rf.model1$finalModel)  # OOB error rate of 0.48%

rm(predictors, response)

```

The out of bag estimate of error rate of 0.48% is quite good, so next step
continues on to check the performance of the model on the validation data.


## Part 4) Model Validation

In the code chunk below the validation data are processed in the same way as the
training data. The model is then used to predict on the validation data.

```{r validating}

validation.data <- validation.data %>%  # 1960 obs. of 160 variables
  ConvertWLE() %>% # 1960 obs. of 160 variables
  SubsetWLE()  # 1960 obs. of 54 variables

# Predict on validation data and check performance
rf.model1 %>%
  predict(newdata = validation.data) %>%
  confusionMatrix(reference = validation.data$classe) %>%  # Accuracy is 0.9959
  print()

```

The accuracy of the model on the original training data was high, and the
accuracy was still high on the validation data, suggesting that overfitting
was not a problem here. Additionally, with such a high accuracy and a long
compute time (approximately 1hr) it does not seem worthwhile to fine tune this
model or fit models using other methods. With high performance on both the
training and validation data, the model is ready to be used on the test data.


## Part 5) Predicting on Test Set

In the final code chunk below, the testing data were downloaded and then
processed in the same way as the training data. The model was then used to
predict on the test data, and the prediction results are shown below.

```{r predicting}

# Check if the file exists in the directory before downloading it again
testing.file <- "pml-testing.csv"
if (!file.exists(testing.file)) {
  url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(url, testing.file)
  rm(url)
}

# Load in the raw testing data
raw.test.data <- read.csv(testing.file, stringsAsFactors = F)  # 20 obs. of 160
# Process the testing data the same as before
test.data <- raw.test.data %>%  # 20 obs. of 160 variables
  ConvertWLE() %>% # 20 obs. of 160 variables
  SubsetWLE()  # 20 obs. of 54 variables
rm(na.columns, testing.file)

# Predict on testing data
test.predictions <- predict(rf.model1, newdata = test.data)  # 20 long
print(test.predictions)

```

